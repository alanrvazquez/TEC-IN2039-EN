---
title: Introduction to Linear Regression 
subtitle: "IN2039: Data Visualization for Decision Making"
author: 
  - name: Alan R. Vazquez
    affiliations:
      - name: Department of Industrial Engineering
format: 
  revealjs:
    chalkboard: true
    multiplex: true
    footer: "Tecnológico de Monterrey"
    logo: IN2039_logo.png
    css: style.css
    slide-number: True
    html-math-method: mathjax
execute:
  echo: true
editor: visual
---

## Agenda

1. Introduction

2. Relationship between two numerical variables

3. Simple linear regression

4. Local regression

## Loading the Libraries

Let's load the `ggplot2`, `ggformula`, `readxl`, and `dplyr` libraries in Google Colab and R before starting.

```{r}
#| echo: true
#| output: false

# Nos se te olvide instalar la librería "ggformula" en Google Colab.
# install.packages("ggformula")
library(readxl)
library(ggplot2)
library(ggformula)
library(dplyr)
```

## Example

We will use data from 392 cars, including miles per gallon, number of cylinders, horsepower, weight, acceleration, year, origin, and other variables.

The data is in the file "auto_dataset.xlsx".

```{r}
#| echo: true

auto_data = read_excel("auto_dataset.xlsx") 
auto_data %>% head() 
```

# Relationship Between Two Numerical Variables

## Principle 1: Formulate the Message

Questions we can answer with simple linear regression:

::: incremental
-   Is there a relationship between a response variable and predictors?

-   How strong is the relationship?

-   ¿What is the uncertainty?

-   How precisely can we predict a future outcome?
:::

## 

Is there a relationship between a car's weight and its miles per gallon?

```{r}
#| fig-pos: center
#| echo: false
#| code-fold: true

mi_diagrama = gf_point(mpg ~ weight, data = auto_data, color = "darkblue", size = 2) + labs(x = "Peso (lb)", y = "Millas por galón")
mi_diagrama = mi_diagrama + theme(axis.text=element_text(size=20), axis.title=element_text(size=20),
                                   plot.title=element_text(size=25))
mi_diagrama
```

## Regression Problem

**Objetive**: Objective: Find the best function $f(X)$ of the predictor $X$ that describes the response $Y$.

. . .

Mathematically, we want to establish the following relationship:

$$
Y = f(X) + \epsilon,
$$

where $\epsilon$ is a natural (random) error.

## 

-   EIn practice, it is very difficult to know the true structure of the function $f(X)$.

::: incremental
-   The best we can do is construct an approximation (function) $\hat{f}(X)$.

-   There are several strategies to build $\hat{f}(X)$, one of the most common is:

    1.  Define a simple "structure" or "formula."
    2.  Estimate the elements of the "formula" using the data.
:::

# Simple Linear Regression

## Linear Regression Model

A very common function $f(X)$ to predict a response $Y$ is the  **linear regression model**.

Its mathematical form is:

$$
\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i,
$$

::: incremental
-   Where $i$ is the index of the $n$ observations, and
-   $\hat{Y}_i$ is the predicted value of the actual response $Y_i$ associated with a predictor value $X_i$.
-   The values $\hat{\beta}_0$ and $\hat{\beta}_1$ are called [coefficients]{style="color:darkblue;"} of the model.
:::

## Principle 2: Convert Data into Information

The values of $\hat{\beta}_0$ and $\hat{\beta}_1$ are obtained using the data. Specifically, the formulas for the coefficients are:

$$\hat{\beta}_1 = \frac{ \sum_{i=1}^{n} (Y_i - \bar{Y}) (X_i - \bar{X}) }{\sum_{i=1}^{n} (X_i - \bar{X})^2} \text{  y  } \hat{\beta}_0 = \bar{Y} - \hat{\beta}_1 \bar{X},$$

where $\bar{X} = \sum_{i=1}^n X_i/n$ and $\bar{Y} = \sum_{i=1}^n Y_i/n$.

These formulas are derived from the method of [*least squares*]{style="color:orange;"}.

## Fitting Regression Models in R

To fit a linear regression model, we use the pipe `%>%` command along with the function `gf_lm()`.

```{r}
#| echo: true
#| output: true

gf_point(mpg ~ weight, data = auto_data) %>% gf_lm(mpg ~ weight)
```

## 

We can modify the color, thickness, and line type using the arguments `color`, `linewidth`, and `lty`, respectively, in the `gf_lm()`function.

```{r}
#| echo: true
#| output: true

gf_point(mpg ~ weight, data = auto_data) %>% 
  gf_lm(mpg ~ weight, color = "red", lty = 1, linewidth = 3)
```

## For Our Example

$\hat{Y}_i = 46.32 -0.0076 X_i$

```{r}
#| fig-pos: center
#| echo: false

grafica_modelo <- mi_diagrama %>% gf_lm(color = "red", lty = 1, size = 2)
grafica_modelo
```

## The Formula

$\text{mpg}_i = 46.32 - 0.0076 \times \text{peso}_i$

```{r}
#| fig-pos: center
#| echo: false

grafica_modelo
```

## Interpretation of the Coefficients

> ¿What does $\hat{\beta}_0 = 46.32$ mean?

. . .

$\hat{\beta}_0$ is the average response value when $X_i = 0$.

```{r}
#| fig-pos: center
#| echo: false
#| out-width: 80%

grafica_intercepto <- grafica_modelo + scale_y_continuous(limits = c(0, 50)) + scale_x_continuous(limits = c(0, 5500))
grafica_intercepto %>% gf_hline(yintercept = 46.317364, lty = 2) %>% gf_vline(xintercept = 0, lty = 2)
```

## 

¿Does $\hat{\beta}_0 = 46.32$ make sense?

```{r}
#| fig-pos: center
#| echo: false

grafica_intercepto <- grafica_modelo + scale_y_continuous(limits = c(0, 50)) + scale_x_continuous(limits = c(0, 5500))
grafica_intercepto %>% gf_hline(yintercept = 46.317364, lty = 2) %>% gf_vline(xintercept = 0, lty = 2)
```

. . .

No! Because there are no cars with a weight of 0.

## 

> ¿What does  $\hat{\beta}_1 = - 0.0076$ mean?

. . .

$\hat{\beta}_1$ is the average change in the response when $X_i$ increases by one unit.

```{r}
#| fig-pos: center
#| echo: false
#| out-width: 80%

grafica_modelo
```

## Interpretation of $\hat{\beta}_1$

</br>

</br>

*For every extra pound in a car's weight, the car has an average reduction of 0.0076 miles per gallon.*

## Do all points fall exactly on the line?

</br>

. . .

No! The model has [**errors**]{style="color:darkgreen;"}.

. . .

Technically, the error of the *i*-th observation is given by: $e_i = Y_i - \hat{Y}_i = Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i$.

# In fact ...

The formulas for $\hat{\beta}_0$ and $\hat{\beta}_1$ are obtained by minimizing the sum of squared errors.

Specifically, the [least squares method]{style="color:orange;"} finds $\hat{\beta}_0$ and $\hat{\beta}_1$ by minimizing the function:

\begin{align}
g(\hat{\beta}_0, \hat{\beta}_1) & = \sum_{i=1}^{n} (e_{i})^2 = \sum_{i=1}^{n} (Y_i - (\hat{\beta}_0 + \hat{\beta}_1 X_i ))^2. 
\end{align}

## Inspecting the Errors

The behavior of the errors ($e_i$'s) indicates whether the model is correct or not. If the model is correct, the errors should behave as follows:

::: incremental
1.  On average, they should **be around 0** for each predicted value $\hat{Y}_i$.
2.  They should have **constant dispersion** around each predicted value $\hat{Y}_i$.
3.  They should be **independent** of each other, meaning they should not be related.
:::

## Graphical Analysis of Errors

To evaluate these behaviors, we use two scatter plots of the errors:

-   **Horizontal Axis** = Errors and **Vertical Axis** = Predictions. This plot helps validate the first two assumptions (**errors around 0 and constant dispersion**).

-   **Eje Horizontal** = Errores y **Eje Vertical** = Tiempo en que se tomó la observación. Está gráfica permite validar el tercer supuesto (**independencia**).

## Constant Dispersion

```{r}
#| echo: true
#| code-fold: true


mi_modelo = lm(mpg ~ weight, data = auto_data)
datos_modelo = tibble("Predicciones" = mi_modelo$fitted, 
                           "Errores" = mi_modelo$residuals)

mi_diagrama_residuos = gf_point(Errores ~ Predicciones, data = datos_modelo, color = "darkblue", size = 2) 
mi_diagrama_residuos = mi_diagrama_residuos + theme(axis.text=element_text(size=20), axis.title=element_text(size=20))
mi_diagrama_residuos = mi_diagrama_residuos %>% gf_hline(yintercept = 0, lty = 2)
mi_diagrama_residuos
```

## Independent Errors

```{r}
#| echo: true
#| code-fold: true


mi_modelo = lm(mpg ~ weight, data = auto_data)
datos_modelo = tibble("Tiempo" = 1:nrow(auto_data), 
                           "Errores" = mi_modelo$residuals)

mi_diagrama_residuos = gf_point(Errores ~ Tiempo, data = datos_modelo, color = "darkblue", size = 2) 
mi_diagrama_residuos = mi_diagrama_residuos + theme(axis.text=element_text(size=20), axis.title=element_text(size=20))
mi_diagrama_residuos = mi_diagrama_residuos %>% gf_hline(yintercept = 0, lty = 2)
mi_diagrama_residuos
```

## Comments

-   The two plots **do not** validate the assumptions of the linear regression model.

-   There are methods to correct this, but we will not cover them here.

-   If both assumptions are not validated, then the linear regression model is used only as a trend line or a reference for the data.

## 

-   If both assumptions are validated, then the model can be used to predict the response for new observations and to check if there is a significant relationship between $Y$ and $X$.

-   We will explore more about linear regression in IN1002B and IN1001B.

# Local Regression

## Local Regression

This is a modern alternative to the simple linear regression model for capturing complex relationships between two variables.

[**Basic Idea**]{style="color:darkgray;"}: It fits linear regression models to small subsets of the data. These subsets consist of observations that are close to each other.

The most common method for fitting a local regression model is **LOESS**. We will omit the details of this method here since they require advanced statistical concepts.


## 

In R, we fit a local regression using the`gf_smooth()`function.

```{r}
gf_point(mpg ~ weight, data = auto_data) %>% 
  gf_smooth(mpg ~ weight, method = "loess")
```

## 

We can change the color, thickness, and line type using the `color`, `linewidth`, and `lty`,arguments in the `gf_smooth()`function.

```{r}
gf_point(mpg ~ weight, data = auto_data) %>% 
  gf_smooth(mpg ~ weight, method = "loess",
            color = "red", linewidth = 2, lty = 2)
```

## Another Example

Let's consider the relationship between car acceleration (`acceleration`) and the total volume of all engine cylinders (`displacement`).

```{r}
gf_point(acceleration ~ displacement, data = auto_data)
```

## Linear Regression

```{r}
#| echo: true
#| code-fold: false
gf_point(acceleration ~ displacement, data = auto_data) %>% 
  gf_lm(acceleration ~ displacement)
```

## Local Regression

```{r}
#| echo: true
#| code-fold: false

gf_point(acceleration ~ displacement, data = auto_data) %>% 
  gf_smooth(acceleration ~ displacement, method = "loess")
```

## Discussion

-   The simple linear regression model is easy to interpret due to its structure:

$$
\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i.
$$

-   However, it may be too rigid to capture complex relationships between two variables.

## 

</br>

-   Local regression is flexible and allows capturing complex relationships between two variables.

-   However, it has a low level of interpretability because it does not provide an explicit equation to relate the predictor $X$ to the response $Y$.

# [Regresar a página principal](https://alanrvazquez.github.io/TEC-IN2039-Website/TEC-IN2039-Website.html)
