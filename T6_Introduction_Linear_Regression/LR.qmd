---
title: Introduction to Linear Regression 
subtitle: "IN2039: Data Visualization for Decision Making"
author: 
  - name: Alan R. Vazquez
    affiliations:
      - name: Department of Industrial Engineering
format: 
  revealjs:
    chalkboard: true
    multiplex: true
    footer: "Tecnologico de Monterrey"
    logo: IN2039_logo.png
    css: style.css
    slide-number: True
    html-math-method: mathjax
execute:
  echo: true
editor: visual
---

## Agenda

1.  Introduction

2.  Relationship between two numerical variables

3.  Simple linear regression

4.  Local regression

## Loading the Libraries

Let's import the `pandas`, `matplotlib`, and `seaborn` in Google Colab.

```{python}
#| echo: true
#| output: false

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
```

## Example

</br>

We will use data from 392 cars, including miles per gallon, number of cylinders, horsepower, weight, acceleration, year, origin, and other variables.

The data is in the file "auto_dataset.xlsx".

```{python}
#| echo: true
#| output: false

auto_data = pd.read_excel("auto_dataset.xlsx")
# Set categorical variable.
auto_data[['origin']] = auto_data[['origin']].astype('category')
```

##

```{python}
#| echo: false
#| output: true

auto_data.head(4)
```


# Relationship Between Two Numerical Variables

## Principle 1: Formulate the question or message

</br>

Questions we can answer with simple linear regression:

::: incremental
-   Is there a relationship between a response variable and predictors?

-   How strong is the relationship?

-   What is the uncertainty?

-   How precisely can we predict a future outcome?
:::

## 

Is there a relationship between a car's weight and its miles per gallon?

```{python}
#| fig-align: center
#| echo: false
#| code-fold: true

plt.figure(figsize=(8, 6))
sns.scatterplot(data=auto_data, x="weight", y="mpg", color="darkblue", s=50)
plt.xlabel("Weight (lb)", fontsize=15)
plt.ylabel("Miles per Gallon", fontsize=15)
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)
plt.show()
```

## Regression Problem

</br>

**Objetive**: Objective: Find the best function $f(X)$ of the predictor $X$ that describes the response $Y$.

. . .

Mathematically, we want to establish the following relationship:

$$
Y = f(X) + \epsilon,
$$

where $\epsilon$ is a natural (random) error.

## 

-   In practice, it is very difficult to know the true structure of the function $f(X)$.

::: incremental
-   The best we can do is construct an approximation (function) $\hat{f}(X)$.

-   There are several strategies to build $\hat{f}(X)$, one of the most common is:

    1.  Define a simple "structure" or "formula."
    2.  Estimate the elements of the "formula" using the data.
:::

# Simple Linear Regression

## Linear Regression Model

A very common function $f(X)$ to predict a response $Y$ is the **linear regression model**.

Its mathematical form is:

$$
\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i,
$$

::: incremental
-   Where $i$ is the index of the $n$ observations, and
-   $\hat{Y}_i$ is the predicted value of the actual response $Y_i$ associated with a predictor value $X_i$.
-   The values $\hat{\beta}_0$ and $\hat{\beta}_1$ are called [coefficients]{style="color:darkblue;"} of the model.
:::

## Principle 2: Turn data into information

The values of $\hat{\beta}_0$ and $\hat{\beta}_1$ are obtained using the data. Specifically, the formulas for the coefficients are:

$$\hat{\beta}_1 = \frac{ \sum_{i=1}^{n} (Y_i - \bar{Y}) (X_i - \bar{X}) }{\sum_{i=1}^{n} (X_i - \bar{X})^2} \text{  y  } \hat{\beta}_0 = \bar{Y} - \hat{\beta}_1 \bar{X},$$

where $\bar{X} = \sum_{i=1}^n X_i/n$ and $\bar{Y} = \sum_{i=1}^n Y_i/n$.

These formulas are derived from the method of [*least squares*]{style="color:orange;"}.

## Fitting Regression Models in R

To fit a linear regression model, we use the `regplot()` in **seaborn**.

```{python}
#| fig-align: center
#| echo: true
#| output: true

plt.figure(figsize=(5.8, 3.8))
sns.regplot(data=auto_data, x="weight", y="mpg")
plt.xlabel("Weight (lb)")
plt.ylabel("Miles per Gallon (mpg)")
plt.show()
```

## 

</br>

We can modify the line type, thickness, and color using the arguments `linestyle`, `linewidth`, and `color`, respectively, in the `line_kws` argument of the function.

```{python}
#| echo: true
#| output: false

plt.figure(figsize=(8, 6))
sns.regplot(data=auto_data, x="weight", y="mpg",  
            scatter_kws={"color": "blue"},
            line_kws={"linestyle": "-", "linewidth": 3, "color": "red", 
                      "label": "Linear Fit"})
plt.xlabel("Weight (lb)")
plt.ylabel("Miles per Gallon (mpg)")
plt.legend()
plt.show()
```

## For Our Example

$\hat{Y}_i = 46.32 -0.0076 X_i$

```{python}
#| fig-align: center
#| echo: false

plt.figure(figsize=(8, 6))
sns.regplot(data=auto_data, x="weight", y="mpg",  
            scatter_kws={"color": "blue"},
            line_kws={"linestyle": "-", "linewidth": 3, "color": "red", 
                      "label": "Linear Fit"})
plt.xlabel("Weight (lb)")
plt.ylabel("Miles per Gallon (mpg)")
plt.legend()
plt.show()
```

## The Formula

$\text{mpg}_i = 46.32 - 0.0076 \times \text{peso}_i$

```{python}
#| fig-align: center
#| echo: false

plt.figure(figsize=(8, 6))
sns.regplot(data=auto_data, x="weight", y="mpg",  
            scatter_kws={"color": "blue"},
            line_kws={"linestyle": "-", "linewidth": 3, "color": "red", 
                      "label": "Linear Fit"})
plt.xlabel("Weight (lb)")
plt.ylabel("Miles per Gallon (mpg)")
plt.legend()
plt.show()
```

## Interpretation of the Coefficients

[What does $\hat{\beta}_0 = 46.32$ mean?]{style="color:darkblue;"}

. . .

$\hat{\beta}_0$ is the average response value when $X_i = 0$.

```{python}
#| fig-align: center
#| echo: false
#| out-width: 80%

import numpy as np

fig, ax = plt.subplots()
sns.scatterplot(data=auto_data, x="weight", y="mpg", color="blue", s=50)
# Define the regression line
x_vals = np.linspace(0, 6000, 100)
y_vals = 46.32 + (-0.0076 * x_vals)

# Add the regression line
plt.plot(x_vals, y_vals, color="red", linestyle="-", label="y = 46.32 - 0.0076x")

ax.set_ylim(0, 50)
ax.set_xlim(0, 5500)

ax.axhline(y=46.317364, linestyle="dashed", color="black")
ax.axvline(x=0, linestyle="dashed", color="black")

plt.xlabel("Weight (lb)", fontsize=15)
plt.ylabel("Miles per Gallon", fontsize=15)
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)

plt.show()
```

## 

Does $\hat{\beta}_0 = 46.32$ make sense?

```{python}
#| fig-align: center
#| echo: false

fig, ax = plt.subplots()
sns.scatterplot(data=auto_data, x="weight", y="mpg", color="blue", s=50)
# Define the regression line
x_vals = np.linspace(0, 6000, 100)
y_vals = 46.32 + (-0.0076 * x_vals)

# Add the regression line
plt.plot(x_vals, y_vals, color="red", linestyle="-", label="y = 46.32 - 0.0076x")

ax.set_ylim(0, 50)
ax.set_xlim(0, 5500)

ax.axhline(y=46.317364, linestyle="dashed", color="black")
ax.axvline(x=0, linestyle="dashed", color="black")

plt.xlabel("Weight (lb)", fontsize=15)
plt.ylabel("Miles per Gallon", fontsize=15)
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)

plt.show()
```

. . .

No! Because there are no cars with a weight of 0.

## 

[What does $\hat{\beta}_1 = - 0.0076$ mean?]{style="color:darkblue;"}

. . .

$\hat{\beta}_1$ is the average change in the response when $X_i$ increases by one unit.

```{python}
#| fig-align: center
#| echo: false
#| out-width: 80%

plt.figure(figsize=(7.5, 5.5))
sns.regplot(data=auto_data, x="weight", y="mpg",  
            scatter_kws={"color": "blue"},
            line_kws={"linestyle": "-", "linewidth": 3, "color": "red", 
                      "label": "Linear Fit"})
plt.xlabel("Weight (lb)")
plt.ylabel("Miles per Gallon (mpg)")
plt.legend()
plt.show()
```

## Interpretation of $\hat{\beta}_1$

</br>

</br>

*For every extra pound in a car's weight, the car has an average reduction of 0.0076 miles per gallon.*

## Do all points fall exactly on the line?

</br>

. . .

No! The model has [**errors**]{style="color:darkgreen;"}.

. . .

Technically, the error of the *i*-th observation is given by: $e_i = Y_i - \hat{Y}_i = Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i$.

## In fact ...

</br>

The formulas for $\hat{\beta}_0$ and $\hat{\beta}_1$ are obtained by minimizing the sum of squared errors.

Specifically, the [least squares method]{style="color:orange;"} finds $\hat{\beta}_0$ and $\hat{\beta}_1$ by minimizing the function:

\begin{align}
g(\hat{\beta}_0, \hat{\beta}_1) & = \sum_{i=1}^{n} (e_{i})^2 = \sum_{i=1}^{n} (Y_i - (\hat{\beta}_0 + \hat{\beta}_1 X_i ))^2. 
\end{align}

## Inspecting the Errors

The behavior of the errors ($e_i$'s) indicates whether the model is correct or not. If the model is correct, the errors should behave as follows:

::: incremental
1.  On average, they should **be around 0** for each predicted value $\hat{Y}_i$.
2.  They should have **constant dispersion** around each predicted value $\hat{Y}_i$.
3.  They should be **independent** of each other, meaning they should not be related.
:::

## Graphical Analysis of Errors

To evaluate these behaviors, we use two scatter plots of the errors:

-   **Horizontal Axis** = Errors and **Vertical Axis** = Predictions. This plot helps validate the first two assumptions (**errors around 0 and constant dispersion**).

-   **Eje Horizontal** = Errores y **Eje Vertical** = Tiempo en que se tom칩 la observaci칩n. Est치 gr치fica permite validar el tercer supuesto (**independencia**).

## statsmodels library

-   **statsmodels** is a powerful python library for statistical modeling, data analysis, and hypothesis testing.
-   It provides classes and functions for estimating statistical models.
-   It is built on top of libraries such as **NumPy**, **SciPy**, and **pandas**
-   <https://www.statsmodels.org/stable/index.html>

![](images/statsmodels-logo-v2-horizontal-dark.svg){fig-align="center"}

## Load the libraries

</br>

Let's import **statsmodels** into Python 

```{python}
#| echo: true
#| output: false

import statsmodels.api as sm
```

## Fit a linear model

</br>

To fit a linear model in **statsmodels**, we first split the data set into a vector with the values of the predictor only, and a vector with the response values.

```{python}
#| echo: true
#| output: true

# Matrix with predictors.
X = auto_data['weight']

# Add intercept.
X = sm.add_constant(X)  

# Matrix with response.
Y = auto_data['mpg']
```

We also must include a vector of ones to the predictors to account for the intercept.

## 

</br></br>

Next, we use the functions `OLS()` and `fit()` from **statsmodels**.

```{python}
#| echo: true
#| output: false

# Create linear regression object
regr = sm.OLS(Y, X)

# Train the model using the training sets
linear_model = regr.fit()
```

## 

</br>

To show the estimated coefficients, we use the argument `params` of the `linear_model` object created previously.

```{python}
#| echo: true
#| output: true

# The estimated coefficients.
print(linear_model.params)
```

The elements in the vector above are the estimates $\hat{\beta}_0 = 46.317$ and $\hat{\beta}_1 = -0.00767$.

## Residuals and fitted values

</br>

To create the plots for the residuals, we need the predicted values ($\hat{Y}_i$'s) of the response and the residuals ($e_i$'s) first. To this end, we use the commands below. 

```{python}
#| echo: true

# Make predictions using the the model
Y_pred = linear_model.fittedvalues

# Calculate residuals.
residuals = linear_model.resid
```


## Constant Dispersion

```{python}
#| echo: true
#| code-fold: true
#| fig-align: center

# Residual vs Fitted Values Plot
plt.figure(figsize=(7.5, 5.5))
sns.scatterplot(x = Y_pred, y = residuals, color="darkblue", s = 50)
plt.axhline(y=0, color='red', linestyle='--')
plt.xlabel('Fitted (predicted) Values', fontsize=15)
plt.ylabel('Residuals', fontsize=15)
ax.tick_params(axis='both', labelsize=12)
plt.show()
```

## Independent Errors

```{python}
#| echo: true
#| code-fold: true
#| fig-align: center

# Residuals vs Time (Case) Plot
plt.figure(figsize=(7.5, 5.5))
sns.scatterplot(x = range(auto_data.shape[0]), y = residuals, 
                color="darkblue", s = 50)
plt.axhline(y=0, color='red', linestyle='--')
plt.xlabel('Time', fontsize=15)
ax.tick_params(axis='both', labelsize=12)
plt.ylabel('Residuals', fontsize=15)
plt.show()
```

## Comments

</br>

</br>

-   The two plots **do not** validate the assumptions of the linear regression model.

-   There are methods to correct this, but we will not cover them here.

-   If both assumptions are not validated, then the linear regression model is used only as a trend line or a reference for the data.

##

</br>

-   If both assumptions are validated, then the model can be used to predict the response for new observations and to check if there is a significant relationship between $Y$ and $X$.

-   You have explored about linear regression in IN1002B.

# Local Regression

## Local Regression

This is a modern alternative to the simple linear regression model for capturing complex relationships between two variables.

[**Basic Idea**]{style="color:darkgray;"}: It fits linear regression models to small subsets of the data. These subsets consist of observations that are close to each other.

The most common method for fitting a local regression model is **LOESS**. We will omit the details of this method here since they require advanced statistical concepts.

## 

In Python, we fit a local regression by setting the `lowess` argument to `True` in  the `regplot()` function. 

```{python}
#| fig-align: center
#| echo: true

plt.figure(figsize=(7, 5))
sns.regplot(data=auto_data, x="weight", y="mpg", lowess=True)
plt.show()
```

## 

We can change the color, thickness, and line type using the `color`, `linewidth`, and `linestyle`,arguments in the `line_kws` argument function. 

```{python}
#| fig-align: center

plt.figure(figsize=(6, 4))
sns.regplot(data=auto_data, x="weight", y="mpg", scatter=True, lowess=True,
            line_kws={"color": "red", "linewidth": 2, "linestyle": "--"})
plt.show()
```

## Another Example

Let's consider the relationship between car acceleration (`acceleration`) and the total volume of all engine cylinders (`displacement`).

```{python}
#| fig-align: center
#| echo: true

plt.figure(figsize=(5.5, 3.5))
sns.scatterplot(data=auto_data, x="displacement", y="acceleration")
plt.show()
```

## Linear Regression

```{python}
#| echo: true
#| fig-align: center

plt.figure(figsize=(7, 5))
sns.regplot(data=auto_data, x="displacement", y="acceleration", 
            scatter=True)
plt.show()
```

## Local Regression

```{python}
#| echo: true
#| fig-align: center

plt.figure(figsize=(7, 5))
sns.regplot(data=auto_data, x="displacement", y="acceleration", 
            scatter=True, lowess = True)
plt.show()
```

## Discussion

</br>

-   The simple linear regression model is easy to interpret due to its structure:

$$
\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i.
$$

-   However, it may be too rigid to capture complex relationships between two variables.

## 

</br>

-   Local regression is flexible and allows capturing complex relationships between two variables.

-   However, it has a low level of interpretability because it does not provide an explicit equation to relate the predictor $X$ to the response $Y$.

# [Return to the main page](https://alanrvazquez.github.io/TEC-IN2039-EN/TEC-IN2039-EN.html)
